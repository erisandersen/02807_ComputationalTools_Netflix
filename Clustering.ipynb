{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2299a370",
   "metadata": {},
   "source": [
    "\n",
    "# Clustering & Recommendations on Netflix ðŸŽ¬  \n",
    "**Pipeline:** Cleaning â†’ TFâ€‘IDF â†’ Shingling â†’ MinHash â†’ LSH â†’ Jaccard Filter â†’ Hybrid Cosine (content + genres/country) â†’ Topâ€‘N Recos â†’ Distance Matrix for Clustering\n",
    "\n",
    "> Dieses Notebook basiert auf deinen Codeâ€‘Snippets und ist so strukturiert, dass du es direkt in VS Code ausfÃ¼hren kannst.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdba532",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Setup & Imports\n",
    "> Wenn eine Bibliothek fehlt, installiere sie in deiner aktiven Umgebung (Terminal in VS Code):\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn datasketch scipy mmh3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52885b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import re\n",
    "import mmh3\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string, re\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Load data\n",
    "# Make sure the CSV is in the same folder or adjust the path.\n",
    "df = pd.read_csv(\"netflix_titles.csv\", encoding=\"latin1\", sep=\",\", quotechar='\"', engine=\"python\")\n",
    "print(f\"Raw rows loaded: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e94aed",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Text Cleaning & Normalization\n",
    "- Entfernt Klammern/AnhÃ¤nge aus Titeln, vereinheitlicht Text (lowercase, Satzzeichen raus).\n",
    "- Dedupliziert anhand von normalisiertem Titel **und** bereinigter Beschreibung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Text cleaning\n",
    "def normalize_title(title):\n",
    "    if pd.isna(title):\n",
    "        return ''\n",
    "    return re.sub(r'\\(.*?\\)', '', title).lower().strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['title_normalized'] = df['title'].fillna('').apply(normalize_title)\n",
    "df['title_clean'] = df['title'].fillna('').apply(clean_text)\n",
    "df['description_clean'] = df['description'].fillna('').apply(clean_text)\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates(subset='title_normalized').reset_index(drop=True)\n",
    "df = df.drop_duplicates(subset='description_clean').reset_index(drop=True)\n",
    "print(f\"Data loaded: {len(df)} unique titles after dedup.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a7f6c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Genres & Countries â†’ Multiâ€‘Hot Features\n",
    "- `listed_in` (Genres, kommagetrennt) â†’ Liste\n",
    "- Kombiniert mit `country` â†’ MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f103a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process genres and countries\n",
    "df['genre_list'] = df['listed_in'].apply(lambda x: [g.strip() for g in x.split(',')] if pd.notnull(x) else [])\n",
    "df['combined_features'] = df['genre_list'] + df['country'].fillna('').apply(lambda x: [x])\n",
    "\n",
    "# One-hot encode genres + countries\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_country_matrix = mlb.fit_transform(df['combined_features'])\n",
    "print(\"Genre+Country feature matrix shape:\", genre_country_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfe399",
   "metadata": {},
   "source": [
    "\n",
    "## 4) TFâ€‘IDF auf Beschreibungen & Topâ€‘WÃ¶rter je Titel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['description_clean'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "rows, cols = tfidf_matrix.nonzero()\n",
    "tfidf_words = defaultdict(list)\n",
    "for r, c in zip(rows, cols):\n",
    "    tfidf_words[r].append((feature_names[c], tfidf_matrix[r, c]))\n",
    "\n",
    "top_n = 20\n",
    "def top_words(doc_idx, n=top_n):\n",
    "    words_scores = tfidf_words[doc_idx]\n",
    "    words_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    words = [w for w, _ in words_scores[:n]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['description_tfidf'] = [top_words(i) for i in range(len(df))]\n",
    "print(\"\\nExample top words for first description:\")\n",
    "print(df.loc[0, 'description_tfidf'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971534e6",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Shingling (qâ€‘grams)\n",
    "> StandardmÃ¤ÃŸig `q=1` (Unigramme). Du kannst `q=2` (Bigrams) setzen, um semantische NÃ¤he zu verstÃ¤rken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b13fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Shingling\n",
    "def shingle(q, text):\n",
    "    words = text.split()\n",
    "    return [words[i:i+q] for i in range(len(words)-q+1)]\n",
    "\n",
    "q = 1\n",
    "shingle_vector = [shingle(q, text) for text in df['description_tfidf']]\n",
    "print(\"\\nExample shingles for first description:\")\n",
    "print(shingle_vector[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9181608",
   "metadata": {},
   "source": [
    "\n",
    "## 6) MinHash Signatures (custom, MurmurHash3)\n",
    "> Erzeugt pro Dokument eine MinHashâ€‘Signatur der LÃ¤nge `k`. Der Anteil gleicher Positionen zwischen zwei Signaturen approximiert die **Jaccardâ€‘Ã„hnlichkeit**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def listhash(l, seed):\n",
    "    val = 0\n",
    "    for e in l:\n",
    "        val ^= mmh3.hash(' '.join(e), seed)\n",
    "    return val\n",
    "\n",
    "def minhash_k(shingles, k):\n",
    "    return [min([listhash(shingle, seed) for shingle in shingles]) for seed in range(1, k+1)]\n",
    "\n",
    "k = 50\n",
    "minhash_signatures = np.array([minhash_k(shingles, k) for shingles in shingle_vector])\n",
    "print(\"\\nExample MinHash signature for first doc:\")\n",
    "print(minhash_signatures[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58b38b",
   "metadata": {},
   "source": [
    "\n",
    "## 7) LSH (Bands Ã— Rows) â†’ Kandidatenpaare\n",
    "> Teilt die Signaturen in `bands Ã— rows` (hier 10 Ã— 5) und sammelt Paare, die in mindestens einem Band identisch sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lsh_candidates(signatures, bands, rows):\n",
    "    assert bands * rows == signatures.shape[1], \"bands * rows must equal signature length\"\n",
    "    candidates = set()\n",
    "    n = signatures.shape[0]\n",
    "    \n",
    "    for b in range(bands):\n",
    "        buckets = defaultdict(list)\n",
    "        for i in range(n):\n",
    "            band_sig = tuple(signatures[i, b*rows:(b+1)*rows])\n",
    "            buckets[band_sig].append(i)\n",
    "        for bucket_docs in buckets.values():\n",
    "            if len(bucket_docs) > 1:\n",
    "                for i_idx in range(len(bucket_docs)):\n",
    "                    for j_idx in range(i_idx+1, len(bucket_docs)):\n",
    "                        candidates.add(tuple(sorted((bucket_docs[i_idx], bucket_docs[j_idx]))))\n",
    "    return candidates\n",
    "\n",
    "bands = 10\n",
    "rows = 5\n",
    "candidates = lsh_candidates(minhash_signatures, bands, rows)\n",
    "print(f\"\\nNumber of candidate pairs: {len(candidates)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46510710",
   "metadata": {},
   "source": [
    "\n",
    "## 8) MinHashâ€‘basierte Jaccardâ€‘SchÃ¤tzung & Filter\n",
    "> SchÃ¤tzt die Jaccardâ€‘Ã„hnlichkeit als Anteil Ã¼bereinstimmender Signaturpositionen und filtert Paare mit `threshold`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Jaccard similarity for candidate pairs\n",
    "def jaccard_list(doc1_idx, doc2_idx, signatures):\n",
    "    sig1 = signatures[doc1_idx]\n",
    "    sig2 = signatures[doc2_idx]\n",
    "    matches = np.sum(sig1 == sig2)\n",
    "    return matches / len(sig1)\n",
    "\n",
    "threshold = 0.35\n",
    "similarities = []\n",
    "for i, j in candidates:\n",
    "    sim = jaccard_list(i, j, minhash_signatures)\n",
    "    if sim >= threshold:\n",
    "        similarities.append((i, j, sim))\n",
    "\n",
    "similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "print(f\"\\nTop 5 similar pairs (threshold={threshold}):\")\n",
    "for i, j, sim in similarities[:5]:\n",
    "    print(f\"- {df.loc[i, 'title']} â†” {df.loc[j, 'title']} | similarity: {sim:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb6582",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Empfehlungen: Hybrid aus MinHash & Cosine (TFâ€‘IDF + Genres/Country)\n",
    "- Start mit MinHashâ€‘Treffern.\n",
    "- ErgÃ¤nze durch gewichtete Cosineâ€‘Similarities (0.7 Content, 0.3 Meta).\n",
    "- Stelle sicher, dass jede:r Titel **Topâ€‘N** Empfehlungen hat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8abd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Build recommendations and ensure all movies have top-N\n",
    "recommendations = defaultdict(list)\n",
    "\n",
    "# Fill from MinHash similarities first\n",
    "for i, j, sim in similarities:\n",
    "    if df.loc[i, 'title_normalized'] == df.loc[j, 'title_normalized']:\n",
    "        continue\n",
    "    recommendations[i].append((j, sim))\n",
    "    recommendations[j].append((i, sim))\n",
    "\n",
    "# Calculate cosine similarity for descriptions\n",
    "desc_similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Calculate cosine similarity for genre + country\n",
    "genre_similarity = cosine_similarity(genre_country_matrix)\n",
    "\n",
    "# Combine both: You can adjust weights (e.g., 0.7 for descriptions, 0.3 for genres)\n",
    "cosine_sim = 0.7 * desc_similarity + 0.3 * genre_similarity\n",
    "\n",
    "top_n = 5\n",
    "for i in range(len(df)):\n",
    "    if len(recommendations[i]) < top_n:\n",
    "        sims = cosine_sim[i]\n",
    "        best_idx = np.argsort(sims)[::-1]\n",
    "        added = 0\n",
    "        for j in best_idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            if any(r[0] == j for r in recommendations[i]):\n",
    "                continue\n",
    "            recommendations[i].append((j, float(sims[j])))\n",
    "            added += 1\n",
    "            if added >= (top_n - len(recommendations[i])):\n",
    "                break\n",
    "\n",
    "# Truncate to top-N total\n",
    "for k_idx, recs in recommendations.items():\n",
    "    recommendations[k_idx] = sorted(recs, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "example_idx = np.random.randint(0, len(df))\n",
    "print(f\"\\nFinal recommendations for '{df.loc[example_idx, 'title']}':\")\n",
    "for rec_idx, sim in recommendations[example_idx]:\n",
    "    print(f\"- {df.loc[rec_idx, 'title']} (similarity: {sim:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d66835",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Similarity â†’ Distance for Clustering\n",
    "> Diese Distanzmatrix solltest du fÃ¼r hierarchisches Clustering/DBSCAN verwenden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa055bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_matrix = cosine_sim\n",
    "distance_matrix = 1 - similarity_matrix  # this should be used for the clustering\n",
    "distance_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63189ff3",
   "metadata": {},
   "source": [
    "\n",
    "## 11) (Optional) Hierarchisches Clustering (Subsample)\n",
    "> VollstÃ¤ndige Paarâ€‘Distanzen sind O(nÂ²). FÃ¼r groÃŸe Daten nimm ein Subsample oder berechne nur Topâ€‘k Nachbarn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65107ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional demo on a small subset to avoid O(n^2) blowup\n",
    "subset = min(400, distance_matrix.shape[0])  # adjust as needed\n",
    "if subset >= 3:\n",
    "    Z = linkage(squareform(distance_matrix[:subset, :subset], checks=False), method='average')\n",
    "    labels = fcluster(Z, t=0.7, criterion='distance')\n",
    "    print(\"Cluster labels (first few):\", labels[:20])\n",
    "else:\n",
    "    print(\"Not enough items for clustering demo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
