{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735fbbb9",
   "metadata": {},
   "source": [
    "# Clustering & Recommendations on Netflix  \n",
    "**Pipeline:** Cleaning → TF‑IDF → Shingling → MinHash → LSH → Top‑N Recommendations → Distance Matrix for Clustering → Max’s sections\n",
    "\n",
    "> This notebook was originally made by Ronja and Eris. I continued working on it from around part 11 onward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008e71e",
   "metadata": {},
   "source": [
    "## 1) (Ronja & Eris) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfda7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import re\n",
    "import mmh3\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string, re\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Load data\n",
    "# Make sure the CSV is in the same folder or adjust the path.\n",
    "df = pd.read_csv(\"netflix_titles.csv\", encoding=\"latin1\", sep=\",\", quotechar='\"', engine=\"python\")\n",
    "print(f\"Raw rows loaded: {len(df)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1551f43",
   "metadata": {},
   "source": [
    "\n",
    "## 2) (Ronja & Eris) Text Cleaning & Normalization\n",
    "- Entfernt Klammern/Anhänge aus Titeln, vereinheitlicht Text (lowercase, Satzzeichen raus).\n",
    "- Dedupliziert anhand von normalisiertem Titel **und** bereinigter Beschreibung.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(title):\n",
    "    if pd.isna(title):\n",
    "        return ''\n",
    "    return re.sub(r'\\(.*?\\)', '', title).lower().strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['title_normalized'] = df['title'].fillna('').apply(normalize_title)\n",
    "df['title_clean'] = df['title'].fillna('').apply(clean_text)\n",
    "df['description_clean'] = df['description'].fillna('').apply(clean_text)\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates(subset='title_normalized').reset_index(drop=True)\n",
    "df = df.drop_duplicates(subset='description_clean').reset_index(drop=True)\n",
    "print(f\"Data loaded: {len(df)} unique titles after dedup.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee06a67",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Genres & Countries -> Multi‑Hot Features\n",
    "- `listed_in` (Genres, kommagetrennt) -> Liste\n",
    "- Kombiniert mit `country` -> MultiLabelBinarizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf48683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process genres and countries\n",
    "df['genre_list'] = df['listed_in'].apply(lambda x: [g.strip() for g in x.split(',')] if pd.notnull(x) else [])\n",
    "df['combined_features'] = df['genre_list'] + df['country'].fillna('').apply(lambda x: [x])\n",
    "\n",
    "# One-hot encode genres + countries\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_country_matrix = mlb.fit_transform(df['combined_features'])\n",
    "print(\"Genre+Country feature matrix shape:\", genre_country_matrix.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196d761",
   "metadata": {},
   "source": [
    "## 4) TF‑IDF auf Beschreibungen & Top‑Wörter je Titel\n",
    "> Note: Das hier zählt nicht als einzelne Methode, die man hernehmen könnte. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d150cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['description_clean'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "rows, cols = tfidf_matrix.nonzero()\n",
    "tfidf_words = defaultdict(list)\n",
    "for r, c in zip(rows, cols):\n",
    "    tfidf_words[r].append((feature_names[c], tfidf_matrix[r, c]))\n",
    "\n",
    "top_n = 20\n",
    "def top_words(doc_idx, n=top_n):\n",
    "    words_scores = tfidf_words[doc_idx]\n",
    "    words_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    words = [w for w, _ in words_scores[:n]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['description_tfidf'] = [top_words(i) for i in range(len(df))]\n",
    "print(\"\\nExample top words for first description:\")\n",
    "print(df.loc[0, 'description_tfidf'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68281ca6",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Shingling (q‑grams)\n",
    "> By default q=1 (unigrams). You can set q=2 (bigrams) to strengthen semantic similarity. (Should have been lecture 3 or 4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3795da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(q, text):\n",
    "    words = text.split()\n",
    "    return [words[i:i+q] for i in range(len(words)-q+1)]\n",
    "\n",
    "q = 1\n",
    "shingle_vector = [shingle(q, text) for text in df['description_tfidf']]\n",
    "print(\"\\nExample shingles for first description:\")\n",
    "print(shingle_vector[0][:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b42795",
   "metadata": {},
   "source": [
    "\n",
    "## 6) MinHash Signatures\n",
    "> Erzeugt pro Dokument eine MinHash‑Signatur der Länge `k`. Der Anteil gleicher Positionen zwischen zwei Signaturen approximiert die **Jaccard‑Ähnlichkeit**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d614a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def listhash(l, seed):\n",
    "    val = 0\n",
    "    for e in l:\n",
    "        val ^= mmh3.hash(' '.join(e), seed)\n",
    "    return val\n",
    "\n",
    "def minhash_k(shingles, k):\n",
    "    return [min([listhash(shingle, seed) for shingle in shingles]) for seed in range(1, k+1)]\n",
    "\n",
    "k = 50\n",
    "minhash_signatures = np.array([minhash_k(shingles, k) for shingles in shingle_vector])\n",
    "print(\"\\nExample MinHash signature for first doc:\")\n",
    "print(minhash_signatures[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f09c25",
   "metadata": {},
   "source": [
    "\n",
    "## 7) LSH (Bands × Rows) -> Kandidatenpaare\n",
    "> Teilt die Signaturen in `bands × rows` (hier 10 × 5) und sammelt Paare, die in mindestens einem Band identisch sind. (Das müsste Die VL 3 gewesen sein, wenn ich mich nicht irre)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lsh_candidates(signatures, bands, rows):\n",
    "    assert bands * rows == signatures.shape[1], \"bands * rows must equal signature length\"\n",
    "    candidates = set()\n",
    "    n = signatures.shape[0]\n",
    "    \n",
    "    for b in range(bands):\n",
    "        buckets = defaultdict(list)\n",
    "        for i in range(n):\n",
    "            band_sig = tuple(signatures[i, b*rows:(b+1)*rows])\n",
    "            buckets[band_sig].append(i)\n",
    "        for bucket_docs in buckets.values():\n",
    "            if len(bucket_docs) > 1:\n",
    "                for i_idx in range(len(bucket_docs)):\n",
    "                    for j_idx in range(i_idx+1, len(bucket_docs)):\n",
    "                        candidates.add(tuple(sorted((bucket_docs[i_idx], bucket_docs[j_idx]))))\n",
    "    return candidates\n",
    "\n",
    "bands = 10\n",
    "rows = 5\n",
    "candidates = lsh_candidates(minhash_signatures, bands, rows)\n",
    "print(f\"\\nNumber of candidate pairs: {len(candidates)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6b2d0",
   "metadata": {},
   "source": [
    "\n",
    "## 8) MinHash‑basierte Jaccard‑Schätzung & Filter\n",
    "> Schätzt die Jaccard‑Ähnlichkeit als Anteil übereinstimmender Signaturpositionen und filtert Paare mit `threshold`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_list(doc1_idx, doc2_idx, signatures):\n",
    "    sig1 = signatures[doc1_idx]\n",
    "    sig2 = signatures[doc2_idx]\n",
    "    matches = np.sum(sig1 == sig2)\n",
    "    return matches / len(sig1)\n",
    "\n",
    "threshold = 0.35\n",
    "similarities = []\n",
    "for i, j in candidates:\n",
    "    sim = jaccard_list(i, j, minhash_signatures)\n",
    "    if sim >= threshold:\n",
    "        similarities.append((i, j, sim))\n",
    "\n",
    "similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "print(f\"\\nTop 5 similar pairs (threshold={threshold}):\")\n",
    "for i, j, sim in similarities[:5]:\n",
    "    print(f\"- {df.loc[i, 'title']} ↔ {df.loc[j, 'title']} | similarity: {sim:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39ec2d",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Recommendations: Hybrid of MinHash & Cosine (TF-IDF + Genres/Country)\n",
    "-Start with MinHash matches.\n",
    "-Add weighted cosine similarities (0.7 content, 0.3 metadata).\n",
    "-Ensure that each title has Top-N recommendations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = defaultdict(list)\n",
    "\n",
    "# Fill from MinHash similarities first\n",
    "for i, j, sim in similarities:\n",
    "    if df.loc[i, 'title_normalized'] == df.loc[j, 'title_normalized']:\n",
    "        continue\n",
    "    recommendations[i].append((j, sim))\n",
    "    recommendations[j].append((i, sim))\n",
    "\n",
    "# Calculate cosine similarity for descriptions\n",
    "desc_similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Calculate cosine similarity for genre + country\n",
    "genre_similarity = cosine_similarity(genre_country_matrix)\n",
    "\n",
    "# Combine both: You can adjust weights (e.g., 0.7 for descriptions, 0.3 for genres)\n",
    "cosine_sim = 0.7 * desc_similarity + 0.3 * genre_similarity\n",
    "\n",
    "top_n = 5\n",
    "for i in range(len(df)):\n",
    "    if len(recommendations[i]) < top_n:\n",
    "        sims = cosine_sim[i]\n",
    "        best_idx = np.argsort(sims)[::-1]\n",
    "        added = 0\n",
    "        for j in best_idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            if any(r[0] == j for r in recommendations[i]):\n",
    "                continue\n",
    "            recommendations[i].append((j, float(sims[j])))\n",
    "            added += 1\n",
    "            if added >= (top_n - len(recommendations[i])):\n",
    "                break\n",
    "\n",
    "# Truncate to top-N total\n",
    "for k_idx, recs in recommendations.items():\n",
    "    recommendations[k_idx] = sorted(recs, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "example_idx = np.random.randint(0, len(df))\n",
    "print(f\"\\nFinal recommendations for '{df.loc[example_idx, 'title']}':\")\n",
    "for rec_idx, sim in recommendations[example_idx]:\n",
    "    print(f\"- {df.loc[rec_idx, 'title']} (similarity: {sim:.2f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0519a",
   "metadata": {},
   "source": [
    "\n",
    "## 10) (Ronja & Eris) Similarity -> Distance for Clustering\n",
    "> Later use this distance matrix for hierarchical clustering/DBSCAN.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_matrix = cosine_sim\n",
    "distance_matrix = 1 - similarity_matrix  # this should be used for the clustering\n",
    "distance_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db4672",
   "metadata": {},
   "source": [
    "\n",
    "## 11) (Max) Hierarchical Clustering (Subsample)\n",
    ">Full pairwise distances are O(n²). For large data, take a subsample or compute only the Top-k neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ac036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional demo on a small subset to avoid O(n^2) blowup\n",
    "subset = min(400, distance_matrix.shape[0])  # adjust as needed\n",
    "if subset >= 3: # ensures there are at least 3 objects\n",
    "    Z = linkage(squareform(distance_matrix[:subset, :subset], checks=False), method='average')\n",
    "    labels = fcluster(Z, t=0.7, criterion='distance')\n",
    "    print(\"Cluster labels (first few):\", labels[:10])\n",
    "else:\n",
    "    print(\"Not enough items for clustering demo.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d411a",
   "metadata": {},
   "source": [
    "## 12) (Max)    Make clusters visible\n",
    ">After the clustering, this makes visible which movies are in the clusters. This is also a simple task that can be done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Voraussetzung:\n",
    "# - distance_matrix = 1 - cosine_sim   (cosine_sim = 0.7*desc + 0.3*genre/country)\n",
    "# - df['listed_in'], df['country'], df['description_tfidf'] existieren\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# --- Parameter ---\n",
    "subset_size = min(400, distance_matrix.shape[0])  # anpassen, falls man mehr/weniger will\n",
    "linkage_method = 'average'\n",
    "distance_threshold = 0.7\n",
    "np.random.seed(42)  # für Reproduzierbarkeit (optional)\n",
    "# Verwende die ERSTEN 'subset_size' Einträge; alternativ: zufällige Indizes\n",
    "idx = np.arange(subset_size)  # oder: np.random.choice(df.index, size=subset_size, replace=False)\n",
    "\n",
    "# --- Submatrix extrahieren und clustern ---\n",
    "D_sub = distance_matrix[np.ix_(idx, idx)]\n",
    "Z = linkage(squareform(D_sub, checks=False), method=linkage_method)\n",
    "labels = fcluster(Z, t=distance_threshold, criterion='distance')\n",
    "\n",
    "# --- Labels sicher in df schreiben (nur für Subset-Indizes) ---\n",
    "df['cluster'] = np.nan\n",
    "df.loc[idx, 'cluster'] = labels\n",
    "df['cluster'] = df['cluster'].astype('Int64')  # hübscher nullable-int dtype\n",
    "\n",
    "print(f\"Subset: {subset_size} Titel | Cluster-Methode: {linkage_method} | Distanz-Schwelle: {distance_threshold}\")\n",
    "print(f\"Anzahl Cluster im Subset: {df.loc[idx, 'cluster'].nunique()}\")\n",
    "\n",
    "# --- Hilfsfunktionen für Cluster-Zusammenfassung ---\n",
    "def top_genres(series_listed_in, k=5):\n",
    "    # series_listed_in: Series mit Strings \"Genre1, Genre2, ...\"\n",
    "    tokens = []\n",
    "    for s in series_listed_in.dropna():\n",
    "        tokens.extend([g.strip() for g in s.split(',') if g.strip()])\n",
    "    return Counter(tokens).most_common(k)\n",
    "\n",
    "def top_countries(series_country, k=5):\n",
    "    tokens = []\n",
    "    for s in series_country.dropna():\n",
    "        # Manche Einträge haben mehrere Länder getrennt durch Komma\n",
    "        tokens.extend([c.strip() for c in str(s).split(',') if c.strip()])\n",
    "    return Counter(tokens).most_common(k)\n",
    "\n",
    "def top_words_from_tfidf(series_tfidf, k=10):\n",
    "    # series_tfidf: enthält bereits die Top-Wörter je Dokument als String\n",
    "    tokens = []\n",
    "    for s in series_tfidf.dropna():\n",
    "        tokens.extend(str(s).split())\n",
    "    return Counter(tokens).most_common(k)\n",
    "\n",
    "# --- Pro Cluster: \"wonach geclustert\" (Profile) ---\n",
    "unique_clusters = sorted(df.loc[idx, 'cluster'].dropna().unique())\n",
    "for cid in unique_clusters:\n",
    "    sub = df.loc[(df.index.isin(idx)) & (df['cluster'] == cid)]\n",
    "    size = len(sub)\n",
    "\n",
    "    genres = top_genres(sub['listed_in'], k=5)\n",
    "    countries = top_countries(sub['country'], k=5)\n",
    "    words = top_words_from_tfidf(sub['description_tfidf'], k=10)\n",
    "\n",
    "    print(f\"\\n=== Cluster {cid} | Größe: {size} ===\")\n",
    "    print(\"Top-Genres:   \", \", \".join([f\"{g} ({n})\" for g, n in genres]) or \"—\")\n",
    "    print(\"Top-Länder:   \", \", \".join([f\"{c} ({n})\" for c, n in countries]) or \"—\")\n",
    "    print(\"Top-Wörter:   \", \", \".join([f\"{w} ({n})\" for w, n in words]) or \"—\") # basically same as in TF-IDF. \n",
    "\n",
    "    # Beispiel-Titel\n",
    "    ex_titles = sub['title'].head(5).tolist()\n",
    "    print(\"Beispiele:    \", \"; \".join(ex_titles) if ex_titles else \"—\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f243d",
   "metadata": {},
   "source": [
    "## 13) (Max) Only show the Top-k clusters:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbe615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Shows the Top-5 largest clusters and their most frequent features\n",
    "# (genres, countries, TF-IDF words) + example titles\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# ---------- Parameters ----------\n",
    "subset_size = min(400, distance_matrix.shape[0])  # for demo; increase if needed\n",
    "linkage_method = 'average'\n",
    "distance_threshold = 0.7\n",
    "top_k = 5\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------- Subset indices ----------\n",
    "idx = np.arange(subset_size)  # or: np.random.choice(df.index, size=subset_size, replace=False)\n",
    "\n",
    "# ---------- Clustering (if necessary) ----------\n",
    "needs_cluster = ('cluster' not in df.columns) or (df.loc[idx, 'cluster'].isna().all())\n",
    "if needs_cluster:\n",
    "    D_sub = distance_matrix[np.ix_(idx, idx)]\n",
    "    Z = linkage(squareform(D_sub, checks=False), method=linkage_method)\n",
    "    labels = fcluster(Z, t=distance_threshold, criterion='distance')\n",
    "    df['cluster'] = np.nan\n",
    "    df.loc[idx, 'cluster'] = labels\n",
    "    df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "print(f\"Subset: {subset_size} | Method: {linkage_method} | Threshold: {distance_threshold}\")\n",
    "sub_df = df.loc[idx]  # consider only the subset\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def top_genres(series_listed_in, k=5):\n",
    "    tokens = []\n",
    "    for s in series_listed_in.dropna():\n",
    "        tokens.extend([g.strip() for g in s.split(',') if g.strip()])\n",
    "    return Counter(tokens).most_common(k)\n",
    "\n",
    "def top_countries(series_country, k=5):\n",
    "    tokens = []\n",
    "    for s in series_country.dropna():\n",
    "        tokens.extend([c.strip() for c in str(s).split(',') if c.strip()])\n",
    "    return Counter(tokens).most_common(k)\n",
    "\n",
    "def top_words_from_tfidf(series_tfidf, k=10):\n",
    "    tokens = []\n",
    "    for s in series_tfidf.dropna():\n",
    "        tokens.extend(str(s).split())\n",
    "    return Counter(tokens).most_common(k)\n",
    "\n",
    "# ---------- Determine Top-K largest clusters ----------\n",
    "sizes = sub_df['cluster'].value_counts(dropna=True).sort_values(ascending=False)\n",
    "top_clusters = list(sizes.head(top_k).index)\n",
    "\n",
    "print(f\"Found clusters in subset: {len(sizes)} | Showing Top-{min(top_k, len(sizes))} largest clusters.\")\n",
    "\n",
    "for rank, cid in enumerate(top_clusters, start=1):\n",
    "    sub = sub_df.loc[sub_df['cluster'] == cid]\n",
    "    size = len(sub)\n",
    "\n",
    "    genres = top_genres(sub['listed_in'], k=5)\n",
    "    countries = top_countries(sub['country'], k=5)\n",
    "    words = top_words_from_tfidf(sub['description_tfidf'], k=10)\n",
    "\n",
    "    print(f\"\\n=== #{rank} | Cluster {cid} | Size: {size} ===\")\n",
    "    print(\"Top genres:    \", \", \".join([f\"{g} ({n})\" for g, n in genres]) or \"—\")\n",
    "    print(\"Top countries: \", \", \".join([f\"{c} ({n})\" for c, n in countries]) or \"—\")\n",
    "    print(\"Top words:     \", \", \".join([f\"{w} ({n})\" for w, n in words]) or \"—\")\n",
    "    print(\"Examples:      \", \"; \".join(sub['title'].head(5).tolist()) if size else \"—\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06c976",
   "metadata": {},
   "source": [
    "## 14) (Max) - K-Means auf Embeddings (TF-IDF & Genres)\n",
    "> K-Means works in feature space (vectors). For cosine similarity, K-Means works well when vectors are L2-normalized (TF-IDF may already do this).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dcdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: Build hybrid embedding (TF-IDF + genres/countries).\n",
    "# We weight the genres/countries a bit weaker (e.g. 0.6). Fine-tune as needed.\n",
    "alpha = 0.6\n",
    "X_hybrid = hstack([tfidf_matrix, csr_matrix(genre_country_matrix * alpha)], format='csr')\n",
    "\n",
    "k = 20  # number of clusters (adjust / later determine via score)\n",
    "km = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=2048, n_init='auto')\n",
    "labels_km = km.fit_predict(X_hybrid)\n",
    "\n",
    "df['cluster_kmeans'] = labels_km\n",
    "print(\"K-Means cluster sizes (Top 10):\")\n",
    "print(pd.Series(labels_km).value_counts().head(10))\n",
    "\n",
    "# Small preview per cluster\n",
    "for cid in pd.Series(labels_km).value_counts().index[:5]:\n",
    "    sub = df.loc[df['cluster_kmeans'] == cid, ['title','listed_in','country']].head(5)\n",
    "    print(f\"\\n=== K-Means Cluster {cid} ===\")\n",
    "    print(sub.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10abf75",
   "metadata": {},
   "source": [
    "## 15) (Max&Dogukan) - DBSCAM auf Similarity/ Distance\n",
    ">Is done directly with the distance matrix ⇒ fits the 1-cosine function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Attention: distance_matrix is (n x n) → O(n^2) memory/time.\n",
    "# For large n, use only subset/approx if needed.\n",
    "eps = 0.30        # corresponds to: cosine_sim >= 0.70\n",
    "min_samples = 5\n",
    "\n",
    "db = DBSCAN(metric='precomputed', eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "labels_db = db.fit_predict(distance_matrix)\n",
    "\n",
    "df['cluster_dbscan'] = labels_db\n",
    "print(\"DBSCAN cluster sizes (without noise -1):\")\n",
    "print(pd.Series(labels_db[labels_db != -1]).value_counts().head(10))\n",
    "\n",
    "# Preview per cluster (without noise -1)\n",
    "for cid in pd.Series(labels_db).value_counts().index:\n",
    "    if cid == -1: \n",
    "        continue\n",
    "    sub = df.loc[df['cluster_dbscan'] == cid, ['title','listed_in','country']].head(5)\n",
    "    print(f\"\\n=== DBSCAN Cluster {cid} ===\")\n",
    "    print(sub.to_string(index=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66677b3f",
   "metadata": {},
   "source": [
    "## 16) (Max&Dogukan) DBSCAN directly on embeddings\n",
    "> Saves memory because no distance matrix needs to be created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861533ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# DBSCAN with cosine metric (works on vectors, not precomputed)\n",
    "eps = 0.30         # here it is the cosine distance threshold; closeness of the points to each other\n",
    "min_samples = 5    # minimum number of points => there must be 5 within distance 0.3 for it to count\n",
    "\n",
    "db_vec = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "# DBSCAN is applied to the TF-IDF vectors\n",
    "labels_db_vec = db_vec.fit_predict(tfidf_matrix)   # tfidf_matrix is CSR → ok\n",
    "\n",
    "df['cluster_dbscan_vec'] = labels_db_vec  # storing cluster labels\n",
    "print(\"DBSCAN(Vectors) cluster sizes (without noise -1):\")  # output\n",
    "print(pd.Series(labels_db_vec[labels_db_vec != -1]).value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3fafc",
   "metadata": {},
   "source": [
    "## 17) (Max&Dogukan) Quickly print the Top‑5 clusters\n",
    "> Profiling/preview is reused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "which = 'cluster_kmeans'  # or 'cluster_dbscan' / 'cluster_dbscan_vec'\n",
    "# Select the column whose clusters we want to inspect.\n",
    "\n",
    "# Value counts (cluster sizes); optionally remove noise (-1)\n",
    "sizes = df[which].value_counts(dropna=True)\n",
    "if -1 in sizes.index:\n",
    "    sizes = sizes[sizes.index != -1]\n",
    "\n",
    "if sizes.empty:\n",
    "    print(f\"No clusters found for column '{which}' (only noise or empty).\")\n",
    "else:\n",
    "    clusters_top5 = sizes.index[:5]  # IDs of the top-5 (or fewer) clusters\n",
    "\n",
    "    print(f\"Top {len(clusters_top5)} largest clusters ({which}):\")\n",
    "    for rank, cid in enumerate(clusters_top5, 1):\n",
    "        sub = df.loc[df[which] == cid]\n",
    "\n",
    "        print(f\"=== #{rank} | Cluster {cid} | Size: {len(sub)} ===\")\n",
    "        print(sub[['title','listed_in','country']].head(5).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
