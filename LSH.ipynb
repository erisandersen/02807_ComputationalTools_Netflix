{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239ab997",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd81e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import re\n",
    "import mmh3\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string, re\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "df = pd.read_csv(\"netflix_titles.csv\", encoding=\"latin1\",sep=\",\",quotechar='\"',engine=\"python\")\n",
    "### TODO: ADD PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361c666",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e77ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 8760 unique titles.\n"
     ]
    }
   ],
   "source": [
    "#%% Text cleaning\n",
    "def normalize_title(title):\n",
    "    if pd.isna(title):\n",
    "        return ''\n",
    "    return re.sub(r'\\(.*?\\)', '', title).lower().strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['title_normalized'] = df['title'].fillna('').apply(normalize_title)\n",
    "df['title_clean'] = df['title'].fillna('').apply(clean_text)\n",
    "df['description_clean'] = df['description'].fillna('').apply(clean_text)\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates(subset='title_normalized').reset_index(drop=True)\n",
    "df = df.drop_duplicates(subset='description_clean').reset_index(drop=True)\n",
    "print(f\"Data loaded: {len(df)} unique titles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc34bd1",
   "metadata": {},
   "source": [
    "Genre based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "553a748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process genres and countries\n",
    "df['genre_list'] = df['listed_in'].apply(lambda x: [g.strip() for g in x.split(',')] if pd.notnull(x) else [])\n",
    "df['combined_features'] = df['genre_list'] + df['country'].fillna('').apply(lambda x: [x])\n",
    "\n",
    "# One-hot encode genres + countries\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_country_matrix = mlb.fit_transform(df['combined_features'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8a6e4",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8288e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example top words for first description:\n",
      "kirsten inevitable johnson comical inventive nears stages filmmaker ways end both face death his help father them life as the\n"
     ]
    }
   ],
   "source": [
    "#%% TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['description_clean'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "rows, cols = tfidf_matrix.nonzero()\n",
    "tfidf_words = defaultdict(list)\n",
    "for r, c in zip(rows, cols):\n",
    "    tfidf_words[r].append((feature_names[c], tfidf_matrix[r, c]))\n",
    "\n",
    "top_n = 20\n",
    "def top_words(doc_idx, n=top_n):\n",
    "    words_scores = tfidf_words[doc_idx]\n",
    "    words_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    words = [w for w, _ in words_scores[:n]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['description_tfidf'] = [top_words(i) for i in range(len(df))]\n",
    "print(\"\\nExample top words for first description:\")\n",
    "print(df.loc[0, 'description_tfidf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48226dc8",
   "metadata": {},
   "source": [
    "Shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e75247c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example shingles for first description:\n",
      "[['kirsten'], ['inevitable'], ['johnson'], ['comical'], ['inventive'], ['nears'], ['stages'], ['filmmaker'], ['ways'], ['end']]\n"
     ]
    }
   ],
   "source": [
    "#%% Shingling\n",
    "def shingle(q, text):\n",
    "    words = text.split()\n",
    "    return [words[i:i+q] for i in range(len(words)-q+1)]\n",
    "\n",
    "q = 1\n",
    "shingle_vector = [shingle(q, text) for text in df['description_tfidf']]\n",
    "print(\"\\nExample shingles for first description:\")\n",
    "print(shingle_vector[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97614627",
   "metadata": {},
   "source": [
    "MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85694ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example MinHash signature for first doc:\n",
      "[-1735738829 -1832300011 -2118588981 -1818711025 -2098281750 -1614472792\n",
      " -2084924011 -1984257982 -1372695722 -1559323272 -2089208976 -1960808834\n",
      " -2027474104 -1615755659 -1690528979 -2087023085 -2048304548 -1995669687\n",
      " -1772009878 -2041343340 -1541090568 -2012499769 -2102064348 -2114282030\n",
      " -1659137596 -1589348634 -2130005453 -1825880913 -1994940666 -2073509781\n",
      " -2066368374 -1925363874 -2029035528 -1883797657 -2067436519 -2110145076\n",
      " -1839633183 -1980926784 -1958820688 -1907894037 -2082984586 -2077980288\n",
      " -2141595500 -2082472082 -1219200554 -1805234070 -1855078378 -2055577738\n",
      " -2081648617 -1919529610]\n"
     ]
    }
   ],
   "source": [
    "def listhash(l, seed):\n",
    "    val = 0\n",
    "    for e in l:\n",
    "        val ^= mmh3.hash(' '.join(e), seed)\n",
    "    return val\n",
    "\n",
    "def minhash_k(shingles, k):\n",
    "    return [min([listhash(shingle, seed) for shingle in shingles]) for seed in range(1, k+1)]\n",
    "\n",
    "k = 50\n",
    "minhash_signatures = np.array([minhash_k(shingles, k) for shingles in shingle_vector])\n",
    "print(\"\\nExample MinHash signature for first doc:\")\n",
    "print(minhash_signatures[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab796e2",
   "metadata": {},
   "source": [
    "LSH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd1b7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidate pairs: 190\n"
     ]
    }
   ],
   "source": [
    "def lsh_candidates(signatures, bands, rows):\n",
    "    assert bands * rows == signatures.shape[1], \"bands * rows must equal signature length\"\n",
    "    candidates = set()\n",
    "    n = signatures.shape[0]\n",
    "    \n",
    "    for b in range(bands):\n",
    "        buckets = defaultdict(list)\n",
    "        for i in range(n):\n",
    "            band_sig = tuple(signatures[i, b*rows:(b+1)*rows])\n",
    "            buckets[band_sig].append(i)\n",
    "        for bucket_docs in buckets.values():\n",
    "            if len(bucket_docs) > 1:\n",
    "                for i_idx in range(len(bucket_docs)):\n",
    "                    for j_idx in range(i_idx+1, len(bucket_docs)):\n",
    "                        candidates.add(tuple(sorted((bucket_docs[i_idx], bucket_docs[j_idx]))))\n",
    "    return candidates\n",
    "\n",
    "bands = 10\n",
    "rows = 5\n",
    "candidates = lsh_candidates(minhash_signatures, bands, rows)\n",
    "print(f\"\\nNumber of candidate pairs: {len(candidates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b8ee0",
   "metadata": {},
   "source": [
    "Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6e93139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 similar pairs (threshold=0.35):\n",
      "- InuYasha the Movie 4: Fire on the Mystic Island ↔ Inuyasha the Movie - L'isola del fuoco scarlatto | similarity: 1.00\n",
      "- Seven Souls in the Skull Castle: Season Bird ↔ Seven Souls in the Skull Castle: Season Wind | similarity: 0.96\n",
      "- Seven Souls in the Skull Castle: Season Bird ↔ Seven Souls in the Skull Castle: Season Flower | similarity: 0.94\n",
      "- Seven Souls in the Skull Castle: Season Moon Jogen ↔ Seven Souls in the Skull Castle: Season Bird | similarity: 0.94\n",
      "- Seven Souls in the Skull Castle: Season Moon Jogen ↔ Seven Souls in the Skull Castle: Season Moon Kagen | similarity: 0.94\n"
     ]
    }
   ],
   "source": [
    "#%% Jaccard similarity for candidate pairs\n",
    "def jaccard_list(doc1_idx, doc2_idx, signatures):\n",
    "    sig1 = signatures[doc1_idx]\n",
    "    sig2 = signatures[doc2_idx]\n",
    "    matches = np.sum(sig1 == sig2)\n",
    "    return matches / len(sig1)\n",
    "\n",
    "threshold = 0.35\n",
    "similarities = []\n",
    "for i, j in candidates:\n",
    "    sim = jaccard_list(i, j, minhash_signatures)\n",
    "    if sim >= threshold:\n",
    "        similarities.append((i, j, sim))\n",
    "\n",
    "similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "print(f\"\\nTop 5 similar pairs (threshold={threshold}):\")\n",
    "for i, j, sim in similarities[:5]:\n",
    "    print(f\"- {df.loc[i, 'title']} ↔ {df.loc[j, 'title']} | similarity: {sim:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3cfb0",
   "metadata": {},
   "source": [
    "Build recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3416551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final recommendations for 'Midnight Mass':\n",
      "- Trial 4 (similarity: 0.41)\n",
      "- Making a Murderer (similarity: 0.36)\n",
      "- The Devil Next Door (similarity: 0.35)\n"
     ]
    }
   ],
   "source": [
    "#%% Build recommendations and ensure all movies have top-N\n",
    "recommendations = defaultdict(list)\n",
    "\n",
    "# Fill from MinHash similarities first\n",
    "for i, j, sim in similarities:\n",
    "    if df.loc[i, 'title_normalized'] == df.loc[j, 'title_normalized']:\n",
    "        continue\n",
    "    recommendations[i].append((j, sim))\n",
    "    recommendations[j].append((i, sim))\n",
    "\n",
    "# Calculate cosine similarity for descriptions\n",
    "desc_similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Calculate cosine similarity for genre + country\n",
    "genre_similarity = cosine_similarity(genre_country_matrix)\n",
    "\n",
    "# Combine both: You can adjust weights (e.g., 0.7 for descriptions, 0.3 for genres)\n",
    "cosine_sim = 0.7 * desc_similarity + 0.3 * genre_similarity\n",
    "\n",
    "top_n = 5\n",
    "for i in range(len(df)):\n",
    "    if len(recommendations[i]) < top_n:\n",
    "        sims = cosine_sim[i]\n",
    "        best_idx = np.argsort(sims)[::-1]\n",
    "        added = 0\n",
    "        for j in best_idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            if any(r[0] == j for r in recommendations[i]):\n",
    "                continue\n",
    "            recommendations[i].append((j, float(sims[j])))\n",
    "            added += 1\n",
    "            if added >= (top_n - len(recommendations[i])):\n",
    "                break\n",
    "\n",
    "# Truncate to top-N total\n",
    "for k, recs in recommendations.items():\n",
    "    recommendations[k] = sorted(recs, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "example_idx = np.random.randint(0, len(df))\n",
    "print(f\"\\nFinal recommendations for '{df.loc[5, 'title']}':\")\n",
    "for rec_idx, sim in recommendations[example_idx]:\n",
    "    print(f\"- {df.loc[rec_idx, 'title']} (similarity: {sim:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d36b95",
   "metadata": {},
   "source": [
    "Build Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a6d4b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.99113316, 0.97229944, ..., 0.86713563, 0.86172156,\n",
       "        0.96087197],\n",
       "       [0.99113316, 0.        , 0.92269044, ..., 0.99766908, 1.        ,\n",
       "        1.        ],\n",
       "       [0.97229944, 0.92269044, 0.        , ..., 0.99004171, 0.98954891,\n",
       "        0.93452146],\n",
       "       ...,\n",
       "       [0.86713563, 0.99766908, 0.99004171, ..., 0.        , 0.78204466,\n",
       "        0.99437644],\n",
       "       [0.86172156, 1.        , 0.98954891, ..., 0.78204466, 0.        ,\n",
       "        0.99498107],\n",
       "       [0.96087197, 1.        , 0.93452146, ..., 0.99437644, 0.99498107,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = cosine_sim\n",
    "\n",
    "distance_matrix = 1 - similarity_matrix ## this should be used for the clustering\n",
    "distance_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9ce8e",
   "metadata": {},
   "source": [
    "Export the recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "caf0f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Export recommendations\n",
    "# rec_rows = []\n",
    "# for movie_idx, recs in recommendations.items():\n",
    "#     for rec_idx, sim in recs:\n",
    "#         rec_rows.append({\n",
    "#             'movie_title': df.loc[movie_idx, 'title'],\n",
    "#             'recommended_title': df.loc[rec_idx, 'title'],\n",
    "#             'similarity': sim\n",
    "#         })\n",
    "\n",
    "# recommendations_df = pd.DataFrame(rec_rows)\n",
    "# recommendations_df.to_csv(\"movie_recommendations.csv\", index=False)\n",
    "# print(\"\\nSaved top-N movie recommendations to 'movie_recommendations.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
